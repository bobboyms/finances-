{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados do cartão de crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torcheval\n",
    "# %pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Category</th>\n",
       "      <th>Card type</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>03/04/2023</td>\n",
       "      <td>100 BEER</td>\n",
       "      <td>-112.31</td>\n",
       "      <td>RESTAURANTES</td>\n",
       "      <td>DEBITO</td>\n",
       "      <td>DESPESA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>08/10/2023</td>\n",
       "      <td>7076 SHOP B-CT CURT</td>\n",
       "      <td>-99.25</td>\n",
       "      <td>COMPRAS DIVERSAS</td>\n",
       "      <td>CREDITO</td>\n",
       "      <td>DESPESA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>28/11/2023</td>\n",
       "      <td>K2 COMERCIO -CT DU01/02</td>\n",
       "      <td>-99.99</td>\n",
       "      <td>COMPRAS DIVERSAS</td>\n",
       "      <td>DEBITO</td>\n",
       "      <td>DESPESA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>19/05/2023</td>\n",
       "      <td>PAY -MP *RUEDOCA-19/05</td>\n",
       "      <td>-202.40</td>\n",
       "      <td>RESTAURANTES</td>\n",
       "      <td>DEBITO</td>\n",
       "      <td>DESPESA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>02/05/2023</td>\n",
       "      <td>PAY -KALUNGA CUR-30/04</td>\n",
       "      <td>-48.60</td>\n",
       "      <td>COMPRAS DIVERSAS</td>\n",
       "      <td>DEBITO</td>\n",
       "      <td>DESPESA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date              Description   Price          Category Card type  \\\n",
       "739   03/04/2023                 100 BEER -112.31      RESTAURANTES    DEBITO   \n",
       "1105  08/10/2023      7076 SHOP B-CT CURT  -99.25  COMPRAS DIVERSAS   CREDITO   \n",
       "366   28/11/2023  K2 COMERCIO -CT DU01/02  -99.99  COMPRAS DIVERSAS    DEBITO   \n",
       "872   19/05/2023   PAY -MP *RUEDOCA-19/05 -202.40      RESTAURANTES    DEBITO   \n",
       "823   02/05/2023   PAY -KALUNGA CUR-30/04  -48.60  COMPRAS DIVERSAS    DEBITO   \n",
       "\n",
       "         Type  \n",
       "739   DESPESA  \n",
       "1105  DESPESA  \n",
       "366   DESPESA  \n",
       "872   DESPESA  \n",
       "823   DESPESA  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def read_and_process_file(file_path, card_type):\n",
    "    df = pd.read_csv(file_path, sep=';', decimal=',',\n",
    "                     names=['Date', 'Description', 'Price', 'Category'])\n",
    "    df[\"Card type\"] = card_type\n",
    "    df[\"Price\"] = df[\"Price\"].apply(convert_price)\n",
    "    df[\"Type\"] = df[\"Price\"].apply(lambda x: \"DESPESA\" if x < 0 else \"RECEITA\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_price(price):\n",
    "    return float(re.sub(r'[^\\d.-]', '', price).replace(',', '.')) / 100\n",
    "\n",
    "\n",
    "# Processando os arquivos\n",
    "df_debit = read_and_process_file('raw_data/cc.txt', 'DEBITO')\n",
    "df_credit = read_and_process_file('raw_data/ca.txt', 'CREDITO')\n",
    "\n",
    "# Concatenando e embaralhando os dados\n",
    "df = pd.concat([df_debit, df_credit], ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=40)\n",
    "\n",
    "# Limpando e transformando dados de texto\n",
    "df[\"Description\"] = df[\"Description\"].str.upper().str.strip()\n",
    "df[\"Category\"] = df[\"Category\"].str.upper().str.strip()\n",
    "df[\"Category\"] = df[\"Category\"].replace(\n",
    "    \"ENTRETENIMENTO\", \"LAZER E ENTRETENIMENTO\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazer o aumento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>- PAY POP SACOLAO - 23 10 /</td>\n",
       "      <td>MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7167</th>\n",
       "      <td>PIX MARCIO 26 TRANSF / 06</td>\n",
       "      <td>TRANSFERÊNCIA BANCÁRIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6344</th>\n",
       "      <td>GYMPASSBR GYMPASS</td>\n",
       "      <td>SAÚDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676</th>\n",
       "      <td>PAMONHAS RSCSS - SE - / 24 02</td>\n",
       "      <td>RESTAURANTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>PAY - RODA DAGUA</td>\n",
       "      <td>LAZER E ENTRETENIMENTO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Description                Category\n",
       "2227    - PAY POP SACOLAO - 23 10 /                 MERCADO\n",
       "7167      PIX MARCIO 26 TRANSF / 06  TRANSFERÊNCIA BANCÁRIA\n",
       "6344              GYMPASSBR GYMPASS                   SAÚDE\n",
       "2676  PAMONHAS RSCSS - SE - / 24 02            RESTAURANTES\n",
       "3648               PAY - RODA DAGUA  LAZER E ENTRETENIMENTO"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def augment_text(text, augmenter, num_variations=5):\n",
    "    return [augmenter.augment(text) for _ in range(num_variations)]\n",
    "\n",
    "\n",
    "# Criar um augmenter\n",
    "augmenter = naw.RandomWordAug(action=\"swap\")\n",
    "\n",
    "# Lista para armazenar as linhas do novo DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Iterar sobre o DataFrame original\n",
    "for _, row in df.iterrows():\n",
    "    description = row['Description']\n",
    "    category = row['Category']\n",
    "\n",
    "    # Gerar variações da descrição\n",
    "    augmented_descriptions = augment_text(description, augmenter)\n",
    "\n",
    "    # Adicionar as variações ao DataFrame\n",
    "    for aug_description in augmented_descriptions:\n",
    "        new_row = {'Description': aug_description[0], 'Category': category}\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Criar um novo DataFrame com as variações\n",
    "df_augmented = pd.DataFrame(new_rows)\n",
    "\n",
    "# Concatenar com o DataFrame original\n",
    "df_combined = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# Embaralhar o DataFrame final\n",
    "df_combined = df_combined.sample(frac=1, random_state=40)\n",
    "\n",
    "# Remover colunas desnecessárias\n",
    "df_combined.drop(['Price', 'Date', 'Card type', 'Type'], axis=1, inplace=True)\n",
    "\n",
    "df_combined.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removendo categoria com apenas 1 exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Supondo que df é o seu DataFrame e 'Category' é a coluna com as classes\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Codificando as categorias e contando as ocorrências de cada código\n",
    "category_counts = pd.Series(label_encoder.fit_transform(\n",
    "    df_combined['Category'])).value_counts()\n",
    "\n",
    "# Identificando os códigos das categorias com apenas um exemplo\n",
    "categories_to_remove = category_counts[category_counts == 1].index.tolist()\n",
    "\n",
    "# Filtrando o DataFrame para remover as categorias com apenas um exemplo\n",
    "df_filtered = df_combined[~df_combined['Category'].isin(\n",
    "    label_encoder.inverse_transform(categories_to_remove))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv(\"full_dataset.csv\", decimal=\",\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MERCADO', 'TRANSFERÊNCIA BANCÁRIA', 'SAÚDE', 'RESTAURANTES',\n",
       "       'LAZER E ENTRETENIMENTO', 'SALÁRIO', 'OUTROS', 'SAQUES',\n",
       "       'COMPRAS DIVERSAS', 'TRANSPORTE', 'POUPANÇA E INVESTIMENTOS',\n",
       "       'RECEITA', 'PAGAMENTO CARTÃO', 'TELECOMUNICAÇÕES', 'MORADIA',\n",
       "       'ROUPAS E ACESSÓRIOS', 'MESADA', 'EDUCAÇÃO', 'CUIDADOS COM O PET',\n",
       "       'CUIDADOS PESSOAIS', 'DOAÇÕES OU CONTRIBUIÇÕES', 'SEGUROS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered['Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faz o processamento do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['<unk>', ' ', 'A', 'P', 'R', 'O', 'I', 'S', 'E', 'T', '-', 'C', 'N', 'L', '0', '1', 'U', 'M', 'Y', '/', 'F', 'D', '2', 'B', 'G', 'H', '*', 'V', 'X', '3', 'K', '6', '5', '4', '8', 'Z', '9', '7', 'Q', 'J', '.', 'W', 'Ç', 'Ê', 'Á', 'Ã', 'Í', 'É', 'Ó', 'a', 'e', 'p']\n",
      "Padded tensor sentences:  torch.Size([7746, 32])\n",
      "max_len 32\n",
      "Vocab Size: 52\n",
      "Number of Classes: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thiagoluizrodrigues/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.vocab = None\n",
    "        self.label_encoder = None\n",
    "        self.one_hot_encoder = None\n",
    "        self.max_len = 0\n",
    "        self.num_classes = 0\n",
    "        self.id_to_label = {}\n",
    "\n",
    "    def build_vocab(self):\n",
    "        def yield_tokens(data_iter):\n",
    "            for text in data_iter:\n",
    "                yield from text\n",
    "\n",
    "        self.vocab = build_vocab_from_iterator(\n",
    "            yield_tokens(self.sentences), specials=[\"<unk>\"])\n",
    "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "\n",
    "    def tokenize_and_pad(self):\n",
    "        vectorized_sentences = [self.vocab(list(text))\n",
    "                                for text in self.sentences]\n",
    "        self.max_len = max(len(seq) for seq in vectorized_sentences)\n",
    "\n",
    "        # Ajustar para o próximo número par, se for ímpar\n",
    "        if self.max_len % 2 != 0:\n",
    "            self.max_len += 1\n",
    "\n",
    "        # Adicionando manualmente um token de preenchimento se necessário\n",
    "        padded_sequences = []\n",
    "        for seq in vectorized_sentences:\n",
    "            seq_len = len(seq)\n",
    "            if seq_len < self.max_len:\n",
    "                seq += [self.vocab[\"<pad>\"]] * (self.max_len - seq_len)\n",
    "            padded_sequences.append(torch.tensor(seq))\n",
    "\n",
    "        padded_sentences = torch.nn.utils.rnn.pad_sequence(\n",
    "            padded_sequences,\n",
    "            batch_first=True,\n",
    "            padding_value=self.vocab[\"<pad>\"]\n",
    "        )\n",
    "\n",
    "        return padded_sentences\n",
    "\n",
    "    def encode_labels(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        encoded_labels = self.label_encoder.fit_transform(self.labels)\n",
    "        self.one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "        one_hot_labels = self.one_hot_encoder.fit_transform(\n",
    "            encoded_labels.reshape(-1, 1))\n",
    "        \n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "        return one_hot_labels\n",
    "\n",
    "    def save_label_mappings(self, filepath='production/id_to_label.pkl'):\n",
    "        label_to_id = dict(zip(self.label_encoder.classes_,\n",
    "                           range(len(self.label_encoder.classes_))))\n",
    "        self.id_to_label = {id_: label for label, id_ in label_to_id.items()}\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self.id_to_label, f)\n",
    "\n",
    "    def save_vocab(self, filepath='production/vocab.pkl'):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(text_processor.vocab, f)\n",
    "\n",
    "    def get_vocab_info(self):\n",
    "        return {\n",
    "            \"vocabulary\": self.vocab.get_itos(),\n",
    "            \"max_len\": self.max_len,\n",
    "            \"vocab_size\": len(self.vocab),\n",
    "            \"num_classes\": self.num_classes\n",
    "        }\n",
    "\n",
    "\n",
    "# Uso da classe\n",
    "sentences = df_filtered[\"Description\"]\n",
    "labels = df_filtered['Category']\n",
    "text_processor = TextProcessor(sentences, labels)\n",
    "\n",
    "text_processor.build_vocab()\n",
    "padded_sentences = text_processor.tokenize_and_pad()\n",
    "one_hot_labels = text_processor.encode_labels()\n",
    "text_processor.save_label_mappings()\n",
    "text_processor.save_vocab()\n",
    "vocab_info = text_processor.get_vocab_info()\n",
    "\n",
    "embed_dim = vocab_info[\"max_len\"]  # padded_sentences.size(1)\n",
    "vocab_size = vocab_info[\"vocab_size\"]\n",
    "num_classes = vocab_info[\"num_classes\"]\n",
    "\n",
    "print(\"Vocabulary: \", vocab_info[\"vocabulary\"])\n",
    "print(\"Padded tensor sentences: \", padded_sentences.shape)\n",
    "print(\"max_len\", vocab_info[\"max_len\"])\n",
    "print(\"Vocab Size:\", vocab_size)\n",
    "print(\"Number of Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os datasets de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.25\n",
    "\n",
    "# Dividindo os dados\n",
    "padded_sentences_train, padded_sentences_test, one_hot_labels_train, one_hot_labels_test = train_test_split(\n",
    "    padded_sentences, one_hot_labels, test_size=test_size, random_state=42, stratify=one_hot_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import TextDataset\n",
    "\n",
    "train_dataset = TextDataset(padded_sentences_train, one_hot_labels_train)\n",
    "test_dataset = TextDataset(padded_sentences_test, one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelXT(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[43mvocab_size\u001b[49m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m,\n\u001b[1;32m     10\u001b[0m                 num_heads\u001b[38;5;241m=\u001b[39mnum_heads, dropout\u001b[38;5;241m=\u001b[39mdropout, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, train_dataset, test_dataset, device)\n\u001b[1;32m     13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "from model.device import get_device\n",
    "from model.model import ModelXT\n",
    "from model.train import Trainer\n",
    "\n",
    "num_heads = 8\n",
    "dropout = 0.3\n",
    "num_layers = 2\n",
    "device = get_device()\n",
    "model = ModelXT(vocab_size=vocab_size, embed_dim=embed_dim,\n",
    "                num_heads=num_heads, dropout=dropout, num_classes=num_classes, num_layers=num_layers)\n",
    "\n",
    "trainer = Trainer(model, train_dataset, test_dataset, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "torch.save(model.state_dict(), 'production/model.pth')\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"dropout\": dropout,\n",
    "}\n",
    "\n",
    "# Escrever os dados em um arquivo JSON\n",
    "with open('production/config.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production\n",
    "- Executa o modelo em modo produção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TRANSPORTE', 'MESADA', 'CUIDADOS COM O PET', 'TRANSPORTE', 'TELECOMUNICAÇÕES', 'MERCADO', 'RESTAURANTES', 'RESTAURANTES', 'RESTAURANTES']\n"
     ]
    }
   ],
   "source": [
    "from production.model import production\n",
    "\n",
    "sentences = [\n",
    "            \"UB-ER* -12/14 TR +IP\",\n",
    "             \"PIX TRANSF IS4B3LL\",\n",
    "             \"5H0P PET PAY\",\n",
    "             \"PAY QRS PARAR SEM\",\n",
    "             \"FL3X CL4RO\",\n",
    "             \"PAY 1F00D\",\n",
    "             \"IF-O0D\",\n",
    "             \"PASTEUR\",\n",
    "             \"DOCELANDIA\"\n",
    "             ]\n",
    "outputs = production(sentences=sentences)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
